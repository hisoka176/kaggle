{
 "metadata": {
  "name": "",
  "signature": "sha256:273c0b49f275523c876dd20f79fb2c5a8cbbcd0b2aae228fea4c620163f59f33"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import IPython.nbformat.current as nbf\n",
      "nova = nbf.read(open('my.ipynb','r'),'ipynb')\n",
      "nbf.write(nova,open('my.py','w'),'py')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "test_df.Pclass.value_counts()\n",
      "np.unique(test_df.Age)\n",
      "np.unique(test_df.SibSp)\n",
      "np.unique(test_df.Parch)\n",
      "test_df.loc[test_df.Fare.isnull(),['Fare']]\n",
      "test_new_em = pd.get_dummies(test_df.Embarked,prefix='Embarked')\n",
      "test_df.drop(['Embarked'])\n",
      "test_df.join(test_new_em)\n",
      "test_df.head(3)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train the model\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "train_df = pd.read_csv('data/train.csv',header=0)\n",
      "test_df = pd.read_csv('data/test.csv',header=0)\n",
      "\n",
      "new_test_dm = pd.get_dummies(test_df.Embarked,prefix='Embarked')\n",
      "test_df.drop(['Embarked'],axis=1,inplace=True)\n",
      "test_df = test_df.join(new_test_dm)\n",
      "\n",
      "new_em = pd.get_dummies(train_df.Embarked,prefix='Embarked')\n",
      "train_df.drop(['Embarked'],axis=1,inplace=True)\n",
      "train_df = train_df.join(new_em)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "clf = LogisticRegression()\n",
      "train_data = train_df.loc[:,['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked_C','Embarked_Q','Embarked_S']]\n",
      "train_label = train_df.loc[:,['Survived']]\n",
      "\n",
      "test_data = test_df.loc[:,['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked_C','Embarked_Q','Embarked_S']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "total_data = train_data.append(test_data)\n",
      "\n",
      "new_sex = pd.get_dummies(total_data['Sex'],prefix='Sex')\n",
      "total_data.drop(['Sex'],axis=1,inplace=True)\n",
      "total_data = total_data.join(new_sex)\n",
      "\n",
      "total_data.Age = total_data.Age.fillna(total_data.Age.mode().tolist()[0])\n",
      "total_data.Fare = total_data.Fare.fillna(total_data.Fare.mode().tolist()[0])\n",
      "\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "train_lengh = len(train_data)\n",
      "test_length = len(test_data)\n",
      "\n",
      "total_array_data = total_data.values\n",
      "train_array_data= total_array_data[:train_lengh,:]\n",
      "test_array_data = total_array_data[train_lengh:,:]\n",
      "\n",
      "train_array_label = train_label.values\n",
      "# gridsearch\n",
      "# roc curve\n",
      "# cross validation\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.\n",
      "clf = LogisticRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}